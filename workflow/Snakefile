from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("6.4.1")


##### setup report #####
configfile: "config/config.yaml"


report: "report/workflow.rst"


##### setup singularity #####


# this container defines the underlying OS for each job when using the workflow
# with --use-conda --use-singularity
container: "docker://continuumio/miniconda3"


##### load rules #####


include: "rules/common.smk"
include: "rules/ref.smk"
include: "rules/trim.smk"
include: "rules/qc.smk"
include: "rules/align.smk"
include: "rules/diffexp.smk"
# include: "rules/bam2bakr.smk"


##### load bam2bakR as module #####

module bam2bakr:
    snakefile:
        github("simonlabcode/bam2bakR", path = "workflow/Snakefile", branch="develop")
    config:
        config["bam2bakr"]

use rule * from bam2bakr exclude index and all as b2b_*


#### Modify bam2bakR rules to use fastq2bakR config and functions
use rule sort_filter from bam2bakr as b2b_sort_filter with:
    input:
        "results/merge_bams/{sample}.merged.bam",
    params:
        format = lambda wildcards: "PE" if get_format(wildcards) else "SE",
        shellscript=workflow.source_path("scripts/sort_filter.sh")
    output:
        "results/sf_reads/{sample}.s.sam",
        "results/sf_reads/{sample}_fixed_mate.bam",
        "results/sf_reads/{sample}.f.sam",

use rule htseq_cnt from bam2bakr as b2b_htseq_cnt with:
    input:
        "results/sf_reads/{sample}.s.sam",
        "resources/genome.gtf",
    output:
        "results/htseq/{sample}_tl.bam",
        temp("results/htseq/{sample}_check.txt")
    params: 
        shellscript=workflow.source_path("../scripts/htseq.sh"),
        pythonscript=workflow.source_path("../scripts/count_triple.py"),
        strand = lambda wildcards: "yes" if unique(get_strandedness(units)) == 'forward' else "reverse"

use rule normalize from bam2bakr as b2b_normalize with:
    input:
        expand("results/htseq/{sample}_tl.bam", sample = SAMP_NAMES)
    output:
        "results/normalization/scale"
    params:
        spikename = config["spikename"],
        rscript=workflow.source_path("scripts/normalize.R")

use rule call_snps from bam2bakr as b2b_call_snps with:
    input:
        "resources/genome.fasta",
        expand("results/htseq/{ctl}_tl.bam", ctl = CTL_NAMES)
    params:
        nsamps = nctl,
        shellscript = workflow.source_path("scripts/call_snps.sh")
    output:
        "results/snps/snp.txt",
        "results/snps/snp.vcf",
        temp("results/snps/mkdir.txt"),

use rule maketdf from bam2bakr as b2b_maketdf with:
    input:
        "results/counts/{sample}_counts.csv.gz",
        "results/htseq/{sample}_tl.bam",
	    "results/normalization/scale",
        "resources/genome.fasta",
    params:
        mut_tracks = config["mut_tracks"],
        wsl = config["WSL"],
        normalize = config["normalize"],
        shellscript = workflow.source_path("scripts/tracks.sh"),
        pythonscript = workflow.source_path("scripts/count_to_tracks.py"),
        strandedness = lambda wildcards: "F" if unique(get_strandedness(units)) == 'forward' else "R"
    output:
        temp("results/tracks/{sample}_success.txt"),
        expand("results/tracks/{{sample}}.{mut}.{id}.{strand}.tdf", mut=config["mut_tracks"], id=[0,1,2,3,4,5], strand = ['pos', 'min'])

use rule cnt_muts from bam2bakr as b2b_cnt_muts with:
    input:
        "results/htseq/{sample}_tl.bam",
        "results/snps/snp.txt"
    params:
        format = lambda wildcards: "SE" if get_format(wildcards) else "PE",
        minqual = config["minqual"],
        mut_tracks = config["mut_tracks"],
        strand = lambda wildcards: "F" if unique(get_strandedness(units)) == 'forward' else "R",
        shellscript = workflow.source_path("scripts/mut_call.sh"),
        pythonscript = workflow.source_path("scripts/mut_call.py"),
        awkscript = workflow.source_path("scripts/fragment_sam.awk") 
    output:
        "results/counts/{sample}_counts.csv.gz",
        temp("results/counts/{sample}_check.txt")

use rule makecB from bam2bakr as b2b_makecB with:
    input:
        expand("results/counts/{sample}_counts.csv.gz", sample=SAMP_NAMES)
    params:
        mut_tracks = config["mut_tracks"],
        keepcols = config["keepcols"],
        shellscript = workflow.source_path("scripts/master.sh")
    output:
        "results/cB/cB.csv.gz"



##### target rules #####


rule all:
    input:
        get_final_output(),
        "results/qc/multiqc_report.html",
        "results/pca.svg",
        "results/cB/cB.csv.gz",
        expand("results/tracks/{sample}.{mut}.{id}.{strand}.tdf", sample = SAMP_NAMES, mut=config["mut_tracks"], id=[0,1,2,3,4,5], strand = ['pos', 'min'])